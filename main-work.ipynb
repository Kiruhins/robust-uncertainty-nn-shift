{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10554308,"sourceType":"datasetVersion","datasetId":6529951},{"sourceId":10728555,"sourceType":"datasetVersion","datasetId":6651314},{"sourceId":249640,"sourceType":"modelInstanceVersion","modelInstanceId":213389,"modelId":235040}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc #For manage garbage ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T09:30:57.674148Z","iopub.execute_input":"2025-05-29T09:30:57.674597Z","iopub.status.idle":"2025-05-29T09:30:57.678746Z","shell.execute_reply.started":"2025-05-29T09:30:57.674554Z","shell.execute_reply":"2025-05-29T09:30:57.677787Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import sys\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T09:30:57.680175Z","iopub.execute_input":"2025-05-29T09:30:57.680529Z","iopub.status.idle":"2025-05-29T09:30:57.693515Z","shell.execute_reply.started":"2025-05-29T09:30:57.680495Z","shell.execute_reply":"2025-05-29T09:30:57.692775Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport datasets\nimport numpy as np\nimport pandas as pd\nimport re\nimport string\n\n\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport time\nimport math\nimport nltk\nimport random\nimport os\n\nfrom typing import List, Tuple\n\nfrom torch.nn.utils import clip_grad_norm_\nfrom transformers import AdamW\nimport inspect\n\n\n\n\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom torch.utils.tensorboard import SummaryWriter\nfrom nltk.translate.bleu_score import SmoothingFunction, corpus_bleu\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nfrom transformers import MBartForConditionalGeneration, MBart50Tokenizer, MBartConfig\nfrom nltk.translate import bleu_score\nfrom datasets import load_dataset\nfrom sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\nfrom transformers import default_data_collator\n#from rouge_score import rouge_scorer\n#from nltk.translate.meteor_score import meteor_score\nnltk.download('wordnet')\nnltk.download('punkt')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.linear_model import LogisticRegression\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device \", device)\n\n\n# torch.cuda.set_per_process_memory_fraction(0.8) \n# %env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n\n# from torch.amp import GradScaler, autocast\n #import gc # Add_to_decrease_memory_for_Collab\n# %env CUDA_LAUNCH_BLOCKING=1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T09:30:57.695291Z","iopub.execute_input":"2025-05-29T09:30:57.695477Z","iopub.status.idle":"2025-05-29T09:31:10.092533Z","shell.execute_reply.started":"2025-05-29T09:30:57.695460Z","shell.execute_reply":"2025-05-29T09:31:10.091854Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\ndevice  cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import transformers\nprint(torch.__version__)\nprint(transformers.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T09:31:10.093624Z","iopub.execute_input":"2025-05-29T09:31:10.094168Z","iopub.status.idle":"2025-05-29T09:31:10.098904Z","shell.execute_reply.started":"2025-05-29T09:31:10.094144Z","shell.execute_reply":"2025-05-29T09:31:10.097961Z"}},"outputs":[{"name":"stdout","text":"2.4.1+cu121\n4.44.2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def print_gpu_memory_usage(step):\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated() / (1024**3)\n        reserved = torch.cuda.memory_reserved() / (1024**3)\n        free = torch.cuda.get_device_properties(0).total_memory / (1024**3) - reserved\n        print(f\"{step}: Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB, Free: {free:.2f} GB\")\n    else:\n        print(f\"{step}: CUDA not available\")\n\n\ndef read_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        return file.readlines()\n\nprint_gpu_memory_usage(\"Before loading data\")\n\n\n#dataset_dir = '/kaggle/input/shifts-dataset/'\ndataset_dir = '/kaggle/input/shifts-eval/eval-data/'\n\n\nen_file = os.path.join(dataset_dir, 'reddit_eval.en')\nru_file = os.path.join(dataset_dir, 'reddit_eval.ru')\nmeta_file = os.path.join(dataset_dir, 'reddit_eval_meta.tsv')\n\n\n\nen_lines = read_file(en_file)\nru_lines = read_file(ru_file)\n\n\nmeta_df = pd.read_csv(meta_file, delim_whitespace=True)\n\n\nassert len(en_lines) == len(ru_lines) == len(meta_df), \"Количество строк в файлах не совпадает\"\n\n\ndata = []\nmeta_df.columns = [col.lower() for col in meta_df.columns]\n\nprint(\"Названия столбцов:\", meta_df.columns)\nprint(\"Первые строки DataFrame:\")\nprint(meta_df.head())\n\nprint(f\"Количество строк в en_file: {len(en_lines)}\")\nprint(f\"Количество строк в ru_file: {len(ru_lines)}\")\nprint(f\"Количество строк в meta_file: {len(meta_df)}\")\n\n\nfor en, ru in zip(en_lines, ru_lines):\n    data.append({\n        'en': en.strip(),\n        'ru': ru.strip()\n    })\n\n\ndf = pd.DataFrame(data)\n\n\nfor i in range(0, 10):\n    if i < len(df):\n        print(f\"\\nRow {i+1}:\")\n        print(f\"English: {df.iloc[i]['en']}\")\n        print(f\"Russian: {df.iloc[i]['ru']}\")\n\n\n#print(df.head()) \n#print(df.info()) \n\n# print(df['length_diff'].describe())\n# print(df['is_exact_match'].value_counts())\n\n# df = pd.DataFrame(pairs, columns=['en', 'ru']) # Normal dataset\n\n\n#check_dataset(data, num_samples=10)  \n\nprint_gpu_memory_usage(\"After loading data\")\n\n\n# en_lines = [\n#     \"Please, go to the nearest store.\",\n#     \"Good morning, how are you doing?\",\n#     \"This is an example of a longer sentence for testing purposes.\"\n# ]\n\ndataset = datasets.Dataset.from_pandas(df)\n\n\n# train_size = int(len(dataset) * 0.8)\n# eval_size = len(dataset) - train_size\n\n# dataset_train = dataset.select(range(train_size))\n# dataset_eval = dataset.select(range(train_size, len(dataset))\n\nprint_gpu_memory_usage(\"After loading data\")\n\ndataset = datasets.Dataset.from_pandas(df)\n\ndataset_train = dataset.select(range(0, 60))\ndataset_eval = dataset.select(range(0, 60))\n\nprint(f\"Train dataset size: {len(dataset_train)}\")\nprint(f\"Eval dataset size: {len(dataset_eval)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T09:31:10.099827Z","iopub.execute_input":"2025-05-29T09:31:10.100155Z","iopub.status.idle":"2025-05-29T09:31:10.223818Z","shell.execute_reply.started":"2025-05-29T09:31:10.100099Z","shell.execute_reply":"2025-05-29T09:31:10.223034Z"}},"outputs":[{"name":"stdout","text":"Before loading data: Allocated: 0.00 GB, Reserved: 0.00 GB, Free: 15.89 GB\nНазвания столбцов: Index(['punct', 'missp', 'cap', 'flu', 'slang', 'emoji', 'tags'], dtype='object')\nПервые строки DataFrame:\n   punct  missp  cap  flu  slang  emoji  tags\n0      0      1    0    0      1      0   0.0\n1      0      0    0    0      0      0   0.0\n2      1      0    0    0      1      0   0.0\n3      0      1    0    0      0      0   0.0\n4      1      0    0    0      0      0   0.0\nКоличество строк в en_file: 3065\nКоличество строк в ru_file: 3065\nКоличество строк в meta_file: 3065\n\nRow 1:\nEnglish: I’ll prolly be picking up the i Sport!\nRussian: Наверное, я возьму iSport!\n\nRow 2:\nEnglish: My mind just melted after seeing this, I really want to be able to look like you someday.\nRussian: Я просто растаял, увидев это, мне бы реально хотелось когда-нибудь выглядеть так, как ты.\n\nRow 3:\nEnglish: Such a cute clitty, and pussy you have\nRussian: У тебя такие милые клиторок и киска.\n\nRow 4:\nEnglish: Spider King in SKT - 2headed spider that has advantage on certain saves.\nRussian: Король-паук в SKT–2 — это двухголовый паук, у которого есть бонусы по некоторым спасброскам.\n\nRow 5:\nEnglish: The usual recommendations are Mirai Nikki, Higurashi, Another and Deadman Wonderland.\nRussian: Обычно рекомендуют Mirai Nikki, Higurashi, Another и Deadman Wonderland.\n\nRow 6:\nEnglish: Also, Bryhildr in the Darkness is by the same author and has a plot structure almost identical with EL.\nRussian: Кроме того, у Brynhildr in the Darkness тот же автор и практически такая же структура сюжета, как и у EL.\n\nRow 7:\nEnglish: You can ask a new person for permission with each edge.\nRussian: Каждый раз, когда ты близок к тому, чтобы кончить, ты можешь попросить у нового комментатора разрешения.\n\nRow 8:\nEnglish: If you get denied (or ignored) by every commenter on this thread you must roll a dice and edge that many more times before you can cum.\nRussian: Если тебе откажут (или тебя проигнорируют) все комментаторы в этой ветке, ты должен бросить кости и довести себя почти до пика столько раз, сколько выпадет на костях, перед тем как сможешь кончить.\n\nRow 9:\nEnglish: ambery hazel\nRussian: янтарно-карие\n\nRow 10:\nEnglish: Most likely it is within 40m of a house and someone complained about people walking near their house.\nRussian: Скорее всего, это где-то в пределах 40 м от дома и кто-то пожаловался, что возле их дома ходят люди.\nAfter loading data: Allocated: 0.00 GB, Reserved: 0.00 GB, Free: 15.89 GB\nAfter loading data: Allocated: 0.00 GB, Reserved: 0.00 GB, Free: 15.89 GB\nTrain dataset size: 60\nEval dataset size: 60\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"tokenizer = MBart50Tokenizer.from_pretrained('facebook/mbart-large-50', src_lang=\"en_XX\", tgt_lang=\"ru_RU\", return_tensors=\"pt\")\n\ndef collate_fn(batch):\n    return default_data_collator(batch)\n\ndef preprocess_function(examples):\n    inputs = examples['en']\n    targets = examples['ru']\n    model_inputs = tokenizer(inputs, text_target=targets,\n                                padding=True, truncation=True)\n    return model_inputs\n    \nmodel_inputs_train = dataset_train.map(preprocess_function,\n                                            batched=True)\nmodel_inputs_test = dataset_eval.map(preprocess_function,\n                                            batched=True)\n\ntrain_loader = DataLoader(dataset=model_inputs_train,\n                            collate_fn=collate_fn, batch_size=10, pin_memory=True, drop_last=True)\nvalidation_loader = DataLoader(dataset=model_inputs_test,\n                                collate_fn=collate_fn, batch_size=10, pin_memory=True, drop_last=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T09:31:10.224586Z","iopub.execute_input":"2025-05-29T09:31:10.224833Z","iopub.status.idle":"2025-05-29T09:31:12.860588Z","shell.execute_reply.started":"2025-05-29T09:31:10.224801Z","shell.execute_reply":"2025-05-29T09:31:12.859644Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/531 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2de0b4ad3f9a4cc49f7dd7a7ec2d7d9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7c49fac139f4ea2ad7ba31860304bda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd41e1b239e043efa06d22022dce3be0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"344fa858cf4d455f9c3c06a52da5a087"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/60 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72ec8a18a92d45e880139a66bd375682"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/60 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72cbab88a4664de6bedc94164544895d"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"\ndef extract_attention_uncertainty(text, mbart_model, tokenizer, device, batch_size=8):\n\n    uncertainty_scores = []\n\n    for i in range(0, len(text), batch_size):\n        batch = text[i:i + batch_size]\n        batch_inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()} \n\n        # print_gpu_memory_usage(f\"Before forward pass for batch {i // batch_size + 1}\")\n        with torch.no_grad():\n            outputs = mbart_model(**batch_inputs, output_attentions=True)\n            # print_gpu_memory_usage(f\"After forward pass for batch {i // batch_size + 1}\")\n\n        if not outputs.encoder_attentions:\n            raise ValueError(\"Модель не вернула attention веса\")\n\n        attentions = outputs.encoder_attentions\n        attentions = [attn for attn in attentions if attn is not None] \n\n        if not attentions:\n            continue\n\n        avg_attention = torch.mean(torch.stack(attentions), dim=(0, 1))  # (batch, seq_len, seq_len)\n\n        attention_entropy = -torch.sum(avg_attention * torch.log(torch.clamp(avg_attention, min=1e-6)), dim=-1)  # (batch, seq_len)\n\n        batch_uncertainty_scores = attention_entropy.mean(dim=-1).cpu().numpy()\n        uncertainty_scores.extend(batch_uncertainty_scores)\n\n        del batch_inputs, outputs, attentions, avg_attention, attention_entropy\n        torch.cuda.empty_cache()\n\n    # print_gpu_memory_usage(\"After processing all batches\")\n    return uncertainty_scores\n\n\n# ==========================================\n# 3.\n# ==========================================\n\nclass QEDataset(Dataset):\n    def __init__(self, en_texts, ru_texts, uncertainty_scores, tokenizer, mbart_model, device):\n\n        self.en_texts = en_texts\n        self.ru_texts = ru_texts\n        self.uncertainty_scores = uncertainty_scores\n        self.tokenizer = tokenizer\n        self.mbart_model = mbart_model\n        self.device = device\n\n    def __len__(self):\n        return len(self.en_texts)\n\n    def __getitem__(self, idx):\n        en_text = self.en_texts[idx]\n        ru_text = self.ru_texts[idx]\n        uncertainty = self.uncertainty_scores[idx]\n\n        # Convert uncertainty to tensor if it's a float or numpy float\n        if isinstance(uncertainty, (float, np.float32, np.float64)):\n            uncertainty = torch.tensor(uncertainty, dtype=torch.float32)\n        elif isinstance(uncertainty, torch.Tensor):\n            uncertainty = uncertainty.float()\n        else:\n            raise TypeError(\"Uncertainty score must be a float or a torch.Tensor.\")\n\n        return en_text, ru_text, uncertainty\n\n    def collate_fn(self, batch):\n        en_texts = [item[0] for item in batch]\n        ru_texts = [item[1] for item in batch]\n        uncertainties = torch.stack([item[2] for item in batch])\n\n        return en_texts, ru_texts, uncertainties\n\n\n\nclass QENetWithUncertainty(nn.Module):\n    def __init__(self, hidden_dim=1024, uncertainty_dim=5, output_dim=1):\n\n        super(QENetWithUncertainty, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(hidden_dim + uncertainty_dim, 512),\n            nn.ReLU(),\n            nn.Linear(512, output_dim)\n        )\n\n    def forward(self, features, uncertainties):\n  \n        x = torch.cat([features, uncertainties], dim=1)\n        return self.fc(x)\n\n\n# ==========================================\n# 5.  \n# ==========================================\ndef extract_mbart_features(texts, mbart_model, tokenizer, device, features_file=\"mbart_features.pt\"):\n\n    if os.path.exists(features_file):\n        saved_features = torch.load(features_file, map_location=device)\n        if saved_features.size(0) == len(texts):\n            return saved_features\n\n    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = mbart_model(**inputs, output_hidden_states=True, return_dict=True)\n        last_hidden = outputs.encoder_last_hidden_state  # (batch_size, seq_len, hidden_dim)\n\n    features = last_hidden.mean(dim=1)  # (batch_size, hidden_dim)\n    \n    # Save\n    torch.save(features, features_file)\n    # print(f\"Features saved to {features_file}.\")\n\n    return features\n\n\ndef entropy_from_logits(logits):\n\n    probs = torch.softmax(logits, dim=-1) \n    entropy = -(probs * torch.log(probs.clamp(min=1e-10))).sum(dim=-1).mean(dim=1)\n    return entropy\n\n\n# def refine_translation_with_qe(\n#     text,\n#     mbart_model,\n#     qe_model,\n#     tokenizer,\n#     device,\n#     mbart_uncertainty_weight=1.0,\n#     qe_uncertainty_weight=1.0,\n#     desired_mbart_uncertainty=0.2,\n#     desired_qe_uncertainty=0.2,\n#     fine_tune_steps=10,\n#     learning_rate=1e-5,\n#     threshold=0.7\n# ):\n#     \n#     \n#     \n\n#    \n#     inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n\n#    \n#     with torch.no_grad():\n#         gen_ids = mbart_model.generate(\n#             **inputs,\n#             max_length=200,\n#             num_beams=5,\n#             do_sample=False,\n#             forced_bos_token_id=tokenizer.lang_code_to_id[\"ru_RU\"],\n#             repetition_penalty=1.1\n#         )\n#     translation = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n\n#     \n#     mbart_model.eval()\n#     with torch.no_grad():\n#         outputs = mbart_model(\n#             **inputs,\n#             output_attentions=True,\n#             output_hidden_states=True,\n#             return_dict=True\n#         )\n\n#    \n#     # last_hidden = outputs.encoder_last_hidden_state  # shape: [1, seq_len, hidden_dim]\n#     # features = last_hidden.mean(dim=1)  # shape: [1, hidden_dim]\n\n#     features = extract_mbart_features(text, mbart_model, tokenizer, device)\n\n#     \n#     attentions = outputs.encoder_attentions  # список тензоров: [num_layers] каждый shape: [1, num_heads, seq_len, seq_len]\n#     avg_attention = torch.mean(torch.stack(attentions), dim=(0, 1))  # shape: [1, seq_len, seq_len]\n#     # attn_entropy = -torch.sum(avg_attention * torch.log(avg_attention + 1e-9), dim=-1).mean()\n\n#     attn_entropy = extract_attention_uncertainty([text], mbart_model, tokenizer, device, batch_size=8)\n\n#     print()\n#     print()\n#     print(\"attn_entropy \", attn_entropy) #mc_dropout_val  tensor([0.7919], device='cuda:0')\n\n#     print()\n#     print()\n\n#     \n#     attn_variance = avg_attention.var(dim=-1)  # shape: [1, seq_len]\n#     attn_variance_mean = attn_variance.mean()  # скаляр\n#     attn_variance_first = attn_variance[0].mean().item() \n\n#    \n#    \n#     \n#     logits = outputs.logits if hasattr(outputs, \"logits\") else None\n#     if logits is not None:\n#         # energy_val = energy_score_compute(logits).mean().item()\n#         entropy_val = entropy_from_logits(logits).mean().item()\n#     else:\n#         # energy_val = 0.0\n#         entropy_val = 0.0\n\n#     \n#     print()\n#     print(f\"entropy_val: {entropy_from_logits(logits).mean().item()}\")\n#     print()\n#     print()\n    \n#     \n#     \n    \n#     (translated_ids, scores_dropout,\n#      entropy_scores_demo, attention_variances_demo,\n#      hidden_variances_demo) = mc_dropout_inference(model, inputs, n=10, batch_size=2, device=device)\n\n\n#     mc_dropout_val = scores_dropout\n#     print(\"mc_dropout_val \", mc_dropout_val) #mc_dropout_val  tensor([0.7919], device='cuda:0')\n#     # print()\n#     # print()\n\n    \n#     \n#     uncertainty_values = [\n#         mc_dropout_val,\n#         # energy_val,\n#         float(entropy_val),\n#         float(attn_variance_mean),\n#         float(attn_variance_first)\n#     ]\n#     uncertainty_tensor = torch.tensor([uncertainty_values], dtype=torch.float32).to(device)\n\n#    \n#     with torch.no_grad():\n#         qe_uncertainty = qe_model(features, uncertainty_tensor)\n\n#     print(\"qe_uncertainty \", qe_uncertainty) #qe_uncertainty  tensor([[1.8887]], device='cuda:0')\n#     print()\n#     print()\n#     \n#     combined_uncertainty = 0.7 * float(attn_entropy[0]) + 0.3 * qe_uncertainty.item()\n\n#     # print(f\"Text: {text}\")\n#     # print(f\"Translation before fine-tuning: {translation}\")\n#     print(f\"MBart attention-entropy: {float(attn_entropy[0]):.4f}, QE uncertainty: {qe_uncertainty.item():.4f}\")\n#     print(f\"Combined uncertainty: {combined_uncertainty:.4f}\")\n\n#     \n#     if combined_uncertainty > threshold:\n#         print(\"High uncertainty detected. Initiating fine-tuning.\")\n#         \n#         mbart_model.train()\n    \n#         \n#         for p in mbart_model.parameters():\n#             p.requires_grad = False\n    \n#         \n#         for layer in mbart_model.model.encoder.layers[-3:]:\n#             for p in layer.parameters():\n#                 p.requires_grad = True\n    \n#         \n#         for layer in mbart_model.model.decoder.layers:\n#             for p in layer.encoder_attn.parameters():\n#                 p.requires_grad = True\n#             for p in layer.encoder_attn_layer_norm.parameters():\n#                 p.requires_grad = True\n\n#         \n#         qe_model.eval()\n#         for p in qe_model.parameters():\n#             p.requires_grad = False\n\n#      \n#         # optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, mbart_model.parameters()), lr=learning_rate)\n#         trainable_params = [p for p in mbart_model.parameters() if p.requires_grad]\n#         optimizer = torch.optim.Adam(trainable_params, lr=learning_rate)\n\n        \n#         criterion = torch.nn.MSELoss()\n\n#         for step in range(fine_tune_steps):\n#             optimizer.zero_grad(set_to_none=True)\n        \n#             out = mbart_model(\n#                 **inputs,\n#                 output_attentions=True,\n#                 output_hidden_states=True,\n#                 return_dict=True\n#             )\n        \n#            \n#             attn_entropy_ft = (\n#                 -torch.stack(out.encoder_attentions).mean(dim=(0, 1))\n#                  .mul(torch.log(torch.stack(out.encoder_attentions).mean(dim=(0, 1)) + 1e-9))\n#                  .sum(dim=-1).mean()\n#             )\n        \n#            \n#             last_h = out.encoder_last_hidden_state.mean(dim=1)\n        \n#             uncertain_tensor = torch.tensor(\n#                 [\n#                     [\n#                         attn_entropy_ft.detach(),     # tensor, градиент не нужен\n#                         entropy_val,                  # фиксированное\n#                         attn_variance_mean,           # фиксированное\n#                         attn_variance_first           # фиксированное\n#                     ]\n#                 ],\n#                 dtype=torch.float32,\n#                 device=device\n#             )\n        \n#             pred_qe_unc = qe_model(last_h, uncertain_tensor)\n        \n#             loss_mbart = criterion(\n#                 attn_entropy_ft.unsqueeze(0),\n#                 torch.as_tensor([desired_mbart_uncertainty], device=device)\n#             )\n#             loss_qe = criterion(\n#                 pred_qe_unc,\n#                 torch.as_tensor([[desired_qe_uncertainty]], device=device)\n#             )\n        \n#             loss = (mbart_uncertainty_weight * loss_mbart +\n#                     qe_uncertainty_weight * loss_qe)\n        \n#             loss.backward()\n#             torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)\n#             optimizer.step()\n        \n#             print(f'  Fine-tune step {step+1}/{fine_tune_steps}, Loss = {loss.item():.4f}')\n\n\n#         mbart_model.eval()\n\n#        \n#         with torch.no_grad():\n#             # refined_ids = model.generate(\n#             #         **inputs,\n#             #         max_length=200,\n#             #         num_beams=5,\n#             #         do_sample=False,\n#             #         forced_bos_token_id=tokenizer.lang_code_to_id[\"ru_RU\"],\n#             #         repetition_penalty=1.1\n#             #     )\n\n\n#             # refined_ids = mbart_model.generate(\n#             #     **inputs,\n#             #     max_length=200,\n#             #     num_beams=5,         # ← тот же search, что и в baseline\n#             #     do_sample=False,\n#             #     forced_bos_token_id=tokenizer.lang_code_to_id['ru_RU'],\n#             #     repetition_penalty=1.1\n#             # )\n                \n#             refined_ids = mbart_model.generate(\n#                 **inputs,\n#                 max_length=200,\n#                 num_beams=4,\n#                 do_sample=True,  # <== ключевой флаг\n#                 top_k=50,\n#                 top_p=0.9,\n#                 temperature=0.8,\n#                 num_return_sequences=1,\n#                 forced_bos_token_id=tokenizer.lang_code_to_id[\"ru_RU\"],\n#                 repetition_penalty=1.1\n#             )\n#         translation = tokenizer.decode(refined_ids[0], skip_special_tokens=True)\n#         #print(f\"Refined translation after fine-tuning: {translation}\")\n#     else:\n#         print(\"Uncertainty is within acceptable limits. No fine-tuning performed.\")\n\n#     return translation\n\n\n\nRISK_BUFFER: list[tuple[str, str]] = []     # (src, tgt)\n\ndef refine_translation_with_qe(\n        src_text: str,\n        tgt_text: str,\n        mbart_model,\n        qe_model,\n        tokenizer,\n        device,\n        *,\n        threshold: float = 0.9,\n        buffer_size: int = 64,\n        fine_tune_epochs: int = 2,\n        batch_size: int = 8,\n        learning_rate: float = 1e-5,\n        label_smoothing: float = 0.1,\n        verbose: bool = True\n) -> str:\n   \n    decode_kwargs = dict(\n        max_length=200, num_beams=4, do_sample=True,\n        top_k=50, top_p=0.9, temperature=0.8,\n        num_return_sequences=1,\n        forced_bos_token_id=tokenizer.lang_code_to_id[\"ru_RU\"],\n        repetition_penalty=1.1\n    )\n\n    \n    mbart_model.eval()\n    inputs = tokenizer(src_text, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        base_ids = mbart_model.generate(**inputs, **decode_kwargs)\n    base_translation = tokenizer.decode(base_ids[0], skip_special_tokens=True)\n\n    \n    attn_entropy = extract_attention_uncertainty([src_text], mbart_model, tokenizer, device)[0]\n    features = extract_mbart_features(src_text, mbart_model, tokenizer, device)\n    uncertainty_tensor = torch.tensor([[0.,0.,0.,0.]], device=device)  # если другие фичи не нужны\n    with torch.no_grad():\n        qe_uncert = qe_model(features, uncertainty_tensor).item()\n    risk = 0.7 * attn_entropy.item() + 0.3 * qe_uncert\n    needs_ft = risk > threshold\n\n    \n    with torch.no_grad():\n        temp_ids = mbart_model.generate(**inputs, **decode_kwargs)\n    temp_refined = tokenizer.decode(temp_ids[0], skip_special_tokens=True)\n\n    \n    if verbose:\n        print(\"=\" * 80)\n        print(f\"SRC       : {src_text}\")\n        print(f\"TGT       : {tgt_text}\")\n        print(f\"BASE_MBART: {base_translation}\")\n        print(f\"attention_entropy = {attn_entropy.item():.4f} | \"\n              f\"qe_uncert = {qe_uncert:.4f} | \"\n              f\"combined_risk = {risk:.4f}  -->  \"\n              f\"{'ADD_TO_BUFFER' if needs_ft else 'OK'}\")\n\n        \n        ref_tok  = tokenizer.tokenize(tgt_text)\n        base_tok = tokenizer.tokenize(base_translation)\n        refn_tok = tokenizer.tokenize(temp_refined)\n\n       \n        def safe_sentence_bleu(ref_tok, hyp_tok):\n            if not ref_tok or not hyp_tok:\n                return 0.0\n            smoothing = SmoothingFunction().method4\n            try:\n                return sentence_bleu([ref_tok], hyp_tok, smoothing_function=smoothing)\n            except ZeroDivisionError:\n                return 0.0\n\n        bleu_base = safe_sentence_bleu(ref_tok, base_tok)\n        bleu_ref  = safe_sentence_bleu(ref_tok, refn_tok)\n        print(f\"BLEU       | base: {bleu_base:.3f}  refined: {bleu_ref:.3f}\")\n\n        \n        def safe_prf(true_tok, pred_tok):\n            if not true_tok or not pred_tok:\n                return 0.0, 0.0, 0.0\n            \n            L = min(len(true_tok), len(pred_tok))\n            t, p = true_tok[:L], pred_tok[:L]\n            p_score = precision_score(t, p, average='micro', zero_division=0)\n            r_score = recall_score(   t, p, average='micro', zero_division=0)\n            f1_s    = f1_score(      t, p, average='micro', zero_division=0)\n            return p_score, r_score, f1_s\n\n        prec_b, rec_b, f1_b = safe_prf(ref_tok, base_tok)\n        prec_r, rec_r, f1_r = safe_prf(ref_tok, refn_tok)\n        print(f\"Precision    | base: {prec_b:.3f}  refined: {prec_r:.3f}\")\n        print(f\"Recall    | base: {rec_b:.3f}  refined: {rec_r:.3f}\")\n        print(f\"F1 Score   | base: {f1_b:.3f}  refined: {f1_r:.3f}\")\n\n    \n    if needs_ft:\n        RISK_BUFFER.append((src_text, tgt_text))\n    if len(RISK_BUFFER) >= buffer_size:\n        if verbose:\n            print(f\"\\n▶ fine-tuning on {len(RISK_BUFFER)} risky pairs …\")\n        _run_batch_finetune(\n            mbart_model, tokenizer, device, RISK_BUFFER,\n            epochs=fine_tune_epochs, bs=batch_size,\n            lr=learning_rate, lbl_smth=label_smoothing, verbose=verbose\n        )\n        RISK_BUFFER.clear()\n\n    \n    with torch.no_grad():\n        final_ids = mbart_model.generate(**inputs, **decode_kwargs)\n    refined_translation = tokenizer.decode(final_ids[0], skip_special_tokens=True)\n\n    if verbose:\n        delta = \"(unchanged)\" if refined_translation == base_translation else \"⇒ UPDATED\"\n        print(f\"REFINED_MBART: {refined_translation}  {delta}\\n\")\n\n    return refined_translation\n\n\n\ndef _run_batch_finetune(model, tokenizer, device, pairs,\n                        *, epochs, bs, lr, lbl_smth, verbose):\n    \n    for p in model.parameters(): p.requires_grad = False\n    for l in model.model.encoder.layers[-3:]:\n        for p in l.parameters(): p.requires_grad = True\n    for p in model.model.decoder.parameters(): p.requires_grad = True\n\n    optim = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n    ce_loss_fn = torch.nn.CrossEntropyLoss(\n        ignore_index=tokenizer.pad_token_id,\n        label_smoothing=lbl_smth\n    )\n\n    model.train()\n    for ep in range(epochs):\n        random.shuffle(pairs)\n        for i in range(0, len(pairs), bs):\n            src_batch, tgt_batch = zip(*pairs[i:i+bs])\n            enc = tokenizer(list(src_batch), return_tensors=\"pt\",\n                            padding=True, truncation=True).to(device)\n            with tokenizer.as_target_tokenizer():\n                labels = tokenizer(list(tgt_batch), return_tensors=\"pt\",\n                                   padding=True, truncation=True).input_ids.to(device)\n            enc[\"labels\"] = labels\n\n            optim.zero_grad()\n            out   = model(**enc)\n            loss  = out.loss\n            loss.backward()\n            clip_grad_norm_(model.parameters(), 1.0)\n            optim.step()\n        if verbose:\n            print(f\"  epoch {ep+1}/{epochs}  loss={loss.item():.4f}\")\n    model.eval()\n\n\n\n\n\n\ndef hidden_state_variance(enc_outputs):\n\n    last_hidden = enc_outputs.encoder_last_hidden_state \n    variance = last_hidden.var(dim=1)  \n    return variance.mean(dim=-1) \n\n# def energy_score_compute(logits):\n\n#     return -torch.logsumexp(logits, dim=-1).mean(dim=1) \n\n\ndef compute_attention_variance(attention_weights):\n\n    variance = attention_weights.var(dim=-1) \n    return variance.mean(dim=(1, 2))\n\n\ndef mahalanobis_distance(x, mean, cov):\n\n    if x.dim() != 2 or mean.dim() != 1 or cov.dim() != 2:\n        raise ValueError(\"Неверные размерности входных данных.\")\n    if x.size(1) != mean.size(0) or cov.size(0) != cov.size(1) or cov.size(0) != mean.size(0):\n        raise ValueError(\"Несоответствие размерностей между x, mean и cov.\")\n\n    if torch.isnan(cov).any() or torch.isinf(cov).any():\n        raise ValueError(\"Ковариационная матрица содержит NaN или бесконечные значения.\")\n\n    diff = x - mean \n\n\n    epsilon = 1e-5 \n    cov = cov + epsilon * torch.eye(cov.size(0), device=cov.device)\n\n    inv_cov = pinv_qr(cov)\n\n    left_term = torch.matmul(diff, inv_cov) \n    mahalanobis_sq = torch.sum(left_term * diff, dim=1) \n    distances = torch.sqrt(mahalanobis_sq) \n\n    return distances\n\n\n\ndef mutual_information(logits_list):\n\n    if not logits_list:\n        raise ValueError(\"logits_list is empty.\")\n\n\n    first_shape = logits_list[0].shape\n    if not all(logits.shape == first_shape for logits in logits_list):\n        raise ValueError(\"All tensors in logits_list must have the same shape.\")\n\n\n    probs = torch.stack([torch.softmax(logits.float(), dim=-1) for logits in logits_list])\n\n\n    mean_probs = probs.mean(dim=0) \n\n\n    epsilon = 1e-6 \n    entropy_mean = -(mean_probs * torch.log(mean_probs + epsilon)).sum(dim=-1) \n\n\n    mean_entropy = -(probs * torch.log(probs + epsilon)).sum(dim=-1).mean(dim=0) \n\n\n    mutual_info = entropy_mean - mean_entropy \n\n\n    return mutual_info.mean(dim=-1)  \n\n\n\n\ndef cosine_similarity_score(h, h_train):\n    cos_sim = torch.nn.functional.cosine_similarity(h, h_train)\n    return 1 - cos_sim.mean()\n\n\n\n\ndef pad_sequences(sequences, pad_token_id=1):\n\n    max_length = max(seq.size(-1) for seq in sequences)\n\n    padded_seqs = []\n    for seq in sequences:\n        if seq.dim() == 1:\n            seq = seq.unsqueeze(0)\n\n        seq_length = seq.size(1)\n        if seq_length < max_length:\n            padding = torch.full((seq.size(0), max_length - seq_length), pad_token_id,\n                                 dtype=seq.dtype, device=seq.device)\n            padded_seq = torch.cat([seq, padding], dim=1)\n        else:\n            padded_seq = seq[:, :max_length]\n\n        padded_seqs.append(padded_seq)\n\n    return torch.cat(padded_seqs, dim=0)\n    \ndef enable_dropout(model):\n    for module in model.modules():\n        if isinstance(module, torch.nn.Dropout):\n            module.train()\n\ndef mc_dropout_inference(model, inputs, n=10, batch_size=2, device=\"cuda\"):\n\n    model.eval()\n    enable_dropout(model) \n\n    num_samples = inputs[\"input_ids\"].size(0)\n    predictions, entropy_scores = [], []\n    attention_variances, hidden_variances = [], []\n    uncertainty_scores = []\n\n    # print(f\"Общее количество предложений: {num_samples}\")\n\n    for start in range(0, num_samples, batch_size):\n        end = min(start + batch_size, num_samples)\n        batch_inputs = {k: v[start:end].to(device) for k, v in inputs.items()}\n    \n        batch_predictions = []\n        batch_entropy_scores = []\n        batch_attention_variances = []\n        batch_hidden_variances = []\n    \n        for _ in range(n):\n            with torch.no_grad():\n                # Генерация перевода\n                gen_ids = model.generate(\n                    **batch_inputs,\n                    max_length=200,\n                    num_beams=5,\n                    do_sample=False,\n                    forced_bos_token_id=tokenizer.lang_code_to_id[\"ru_RU\"],\n                    repetition_penalty=1.1\n                )\n    \n                batch_predictions.append(gen_ids.cpu())\n    \n                # Прямой проход модели\n                outputs = model(**batch_inputs, return_dict=True, output_hidden_states=True, output_attentions=True)\n                logits1 = outputs.logits\n    \n                # # Энергетический скор\n                # if callable(energy_score_compute):\n                #     temp = energy_score_compute(logits1).cpu()\n                #     batch_energy_scores.append(temp)\n                # else:\n                #     print(f\"[Warning] energy_score_compute is not callable. Type: {type(energy_score_compute)}\")\n    \n                # Энтропия из логитов\n                batch_entropy_scores.append(entropy_from_logits(logits1).cpu())\n    \n                # Дисперсия скрытых состояний\n                batch_hidden_variances.append(hidden_state_variance(outputs).cpu())\n    \n                # Дисперсия внимания\n                if outputs.encoder_attentions:\n                    attention_weights = torch.stack(outputs.encoder_attentions, dim=0).mean(dim=0)\n                    \n                    if callable(compute_attention_variance):\n                        batch_attention_variances.append(compute_attention_variance(attention_weights).cpu())\n                    else:\n                        print(f\"[Warning] compute_attention_variance is not callable. Type: {type(compute_attention_variance)}\")\n                else:\n                    batch_attention_variances.append(torch.zeros(logits1.shape[0], device=device))\n    \n                # Очистка памяти\n                del gen_ids, outputs, logits1\n                torch.cuda.empty_cache()\n                gc.collect()\n\n        batch_predictions = pad_sequences(batch_predictions, pad_token_id=1)\n\n        # batch_energy_scores = torch.stack(batch_energy_scores).mean(dim=0)\n        batch_entropy_scores = torch.stack(batch_entropy_scores).mean(dim=0)\n        batch_hidden_variances = torch.stack(batch_hidden_variances).mean(dim=0)\n        batch_attention_variances = torch.stack(batch_attention_variances).mean(dim=0)\n\n        batch_uncertainty_scores = (batch_entropy_scores + batch_hidden_variances) / 2\n        uncertainty_scores.append(batch_uncertainty_scores)\n\n        predictions.append(batch_predictions.mode(dim=0)[0])\n        # energy_scores.append(batch_energy_scores)\n        entropy_scores.append(batch_entropy_scores)\n        attention_variances.append(batch_attention_variances)\n        hidden_variances.append(batch_hidden_variances)\n\n        print(f\"Batch {start // batch_size + 1}: обработано {batch_uncertainty_scores.shape[0]} предложений\")\n\n    model.train()\n\n    predictions = pad_sequences(predictions, pad_token_id=1)\n    # energy_scores = torch.cat(energy_scores, dim=0)\n    entropy_scores = torch.cat(entropy_scores, dim=0)\n    attention_variances = torch.cat(attention_variances, dim=0)\n    hidden_variances = torch.cat(hidden_variances, dim=0)\n    uncertainty_scores = torch.cat(uncertainty_scores, dim=0)\n\n    return (\n        predictions.to(device),\n        uncertainty_scores.to(device),\n        # energy_scores.to(device),\n        entropy_scores.to(device),\n        attention_variances.to(device),\n        hidden_variances.to(device)\n    )\n\ndef pinv_qr(A):\n    Q, R = torch.qr(A)\n    return torch.mm(R.pinverse(), Q.t())\n\n\n\n\n\n\ndef create_model_with_dropout(dropout_rate):\n     config = MBartConfig.from_pretrained('facebook/mbart-large-50-many-to-many-mmt',\n                                            dropout=dropout_rate, attention_dropout=dropout_rate)\n     config.output_attentions = True  # Включаем return attentions\n     model_dir = \"/kaggle/input/mbart-large-50-70000to170000-80000to100000-5epochs/pytorch/default/1/results/to/save/model_dropout_result\"\n     model = MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50-many-to-many-mmt', config=config)\n     # model = MBartForConditionalGeneration.from_pretrained(\"path/to/save/model_dropout_0.1\", config=config)\n     # model = MBartForConditionalGeneration.from_pretrained(model_dir, config=config)\n     return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T09:31:12.861588Z","iopub.execute_input":"2025-05-29T09:31:12.861896Z","iopub.status.idle":"2025-05-29T09:31:12.904286Z","shell.execute_reply.started":"2025-05-29T09:31:12.861874Z","shell.execute_reply":"2025-05-29T09:31:12.903562Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\n\nprint_gpu_memory_usage(\"Before loading model\")\n\nmodel = create_model_with_dropout(0.3)\nmodel.to(device)\n\nprint_gpu_memory_usage(\"After loading model\")\n\nru_texts_demo = []\n\nru_texts_file = \"ru_texts_demo.txt\"\n\nif os.path.exists(ru_texts_file):\n    with open(ru_texts_file, \"r\", encoding=\"utf-8\") as f:\n        ru_texts_demo = [line.strip() for line in f.readlines()]\n    if len(ru_texts_demo) != len(en_lines):\n        print(\"File found, but the number of lines does not match. Translations will be regenerated.\")\n        ru_texts_demo = []\nelse:\n    ru_texts_demo = []\n\nprint(\"Number of lines in en_lines:\", len(en_lines))\n\nif not ru_texts_demo:\n    for i, txt in enumerate(en_lines[:100]):\n\n        inputs = tokenizer(txt, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n\n\n        with torch.no_grad():\n            gen_ids = model.generate(\n                **inputs,\n                max_length=200,\n                num_beams=5,  \n                do_sample=False, \n                forced_bos_token_id=tokenizer.lang_code_to_id[\"ru_RU\"], \n                repetition_penalty=1.1\n            )\n\n        ru_translation = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n        ru_texts_demo.append(ru_translation)\n\n        input_text = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n\n\n        if i % 50 == 0:\n            print(f\"Generated {i} sentences\")\n\n        del inputs, gen_ids\n\n    gc.collect()\n\n    with open(ru_texts_file, \"w\", encoding=\"utf-8\") as f:\n        for line in ru_texts_demo:\n            f.write(line + \"\\n\")\n\n    #####-----------------------------------------------\n\n\nif ru_texts_demo:\n    with open(ru_texts_file, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"\\n\".join(ru_texts_demo))\n    print(f\"Translations saved to {ru_texts_file}\")\nelse:\n    print(f\"File {ru_texts_file} found, translations loaded.\")\n\n\n# print_gpu_memory_usage(\"After processing lines\")\n\nindices = random.sample(range(len(ru_texts_demo)), min(5, len(ru_texts_demo)))\n\n\nfor i in indices:\n    print(f\"Input sentence: {en_lines[i]}\")\n    print(f\"Generated MBart translation: {ru_texts_demo[i]}\\n\")\n\n\nscores_demo = extract_attention_uncertainty(en_lines[:100], model, tokenizer, device)\n\n\ninputs = tokenizer(en_lines[:10], padding=True, truncation=True, return_tensors='pt').to(device)\n\n\n(translated_ids, scores_dropout,\n entropy_scores_demo, attention_variances_demo,\n hidden_variances_demo) = mc_dropout_inference(model, inputs, n=10, batch_size=2, device=device)\n\n\nru_texts_demo_mcdropout = tokenizer.batch_decode(\n    [seq for batch in translated_ids.tolist() for seq in batch],  \n    skip_special_tokens=True\n)\n\n\ndataset_demo = QEDataset(en_lines[:100], ru_texts_demo, scores_demo, tokenizer, model, device)\ndataloader_demo = DataLoader(dataset_demo, batch_size=2, shuffle=True, collate_fn=dataset_demo.collate_fn)\n\n\nprint(f\"Size of en_lines[:100]: {len(en_lines[:100])}\")\nprint(f\"Size of ru_texts_demo_mcdropout: {len(ru_texts_demo_mcdropout)}\")\nprint(f\"Size of scores_dropout: {len(scores_dropout)}\")\n\n\nmin_size = min(len(en_lines[:100]), len(ru_texts_demo_mcdropout), len(scores_dropout))\n\nen_lines_trimmed = en_lines[:min_size]\nru_texts_demo_mcdropout_trimmed = ru_texts_demo_mcdropout[:min_size]\nscores_dropout_trimmed = scores_dropout[:min_size]\n\n\ndataset_demo_mcdropout = QEDataset(\n    en_lines_trimmed,\n    ru_texts_demo_mcdropout_trimmed,\n    scores_dropout_trimmed,\n    tokenizer,\n    model,\n    device\n)\n\ndataloader_demo_mcdropout = DataLoader(\n    dataset_demo_mcdropout,\n    batch_size=1,\n    shuffle=True,\n    collate_fn=dataset_demo_mcdropout.collate_fn\n)\n\nprint(f\"Updated: dataset_demo_mcdropout now contains {len(dataset_demo_mcdropout)} examples.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T09:31:12.906005Z","iopub.execute_input":"2025-05-29T09:31:12.906206Z","iopub.status.idle":"2025-05-29T09:32:52.018930Z","shell.execute_reply.started":"2025-05-29T09:31:12.906190Z","shell.execute_reply":"2025-05-29T09:32:52.017974Z"}},"outputs":[{"name":"stdout","text":"Before loading model: Allocated: 0.00 GB, Reserved: 0.00 GB, Free: 15.89 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55f079763c6b4abaa7a6b587abf927de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79818c39efe24bc4b8cc8eb962898bca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c599afc35f674003b304614a94923ef8"}},"metadata":{}},{"name":"stdout","text":"After loading model: Allocated: 2.28 GB, Reserved: 2.29 GB, Free: 13.60 GB\nNumber of lines in en_lines: 3065\nGenerated 0 sentences\nGenerated 50 sentences\nTranslations saved to ru_texts_demo.txt\nInput sentence: This project offers a collateralization rate of 105%.\n\nGenerated MBart translation: Этот проект предлагает процентную гарантию в размере 105 %.\n\nInput sentence: You will definitely make more characters in the future.\n\nGenerated MBart translation: Вы обязательно будете делать больше персонажей в будущем.\n\nInput sentence: Bless Unleashed, Black Desert, Neverwinter, FF14(Playstation), Elder Scrolls Online, Phantasy Star Online 2(My fave), Trove, Skyforge, DC online, Star Trek Online(Traaaash).\n\nGenerated MBart translation: Bless Unleashed, Black Desert, Neverwinter, FF14(Playstation), Elder Scrolls Online, Phantasy Star Online 2(My fave), Trove, Skyforge, DC online, Star Trek Online(Traaaash).\n\nInput sentence: ambery hazel\n\nGenerated MBart translation: янтарная медь\n\nInput sentence: Risperidone seems to be the only antipsychotic besides nuplazid\n\nGenerated MBart translation: Risperidone, кажется, является единственным антипсихотическим кроме нуплазида\n\nBatch 1: обработано 2 предложений\nBatch 2: обработано 2 предложений\nBatch 3: обработано 2 предложений\nBatch 4: обработано 2 предложений\nBatch 5: обработано 2 предложений\nSize of en_lines[:100]: 100\nSize of ru_texts_demo_mcdropout: 225\nSize of scores_dropout: 10\nUpdated: dataset_demo_mcdropout now contains 10 examples.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ==========================================\n# 7. Обучение QE-модели\n# ==========================================\nqe_model = QENetWithUncertainty(hidden_dim=1024, uncertainty_dim=4).to(device)\noptimizer = optim.Adam(qe_model.parameters(), lr=1e-4)\ncriterion = nn.MSELoss()\n\nnum_epochs = 3\n\nprint_gpu_memory_usage(\"Before_QE\")\n\n# Обучение QE\nfor epoch in range(num_epochs):\n    qe_model.train()\n    total_loss = 0\n    \n    for i, (en_batch, ru_batch, uncertainty_batch) in enumerate(dataloader_demo_mcdropout):\n\n        features = extract_mbart_features(en_batch, model, tokenizer, device)\n\n        uncertainty_features = []\n        for j in range(len(en_batch)):\n            dataset_index = i * dataloader_demo_mcdropout.batch_size + j \n\n\n            score_value_mcdropout = scores_dropout[dataset_index].item() if dataset_index < len(scores_dropout) else 0.0\n            #energy_score = energy_scores_demo[dataset_index].item() if dataset_index < len(energy_scores_demo) else 0.0\n            entropy_score = entropy_scores_demo[dataset_index].item() if dataset_index < len(entropy_scores_demo) else 0.0\n            attention_variance_mean = attention_variances_demo[dataset_index].mean().item() if dataset_index < len(attention_variances_demo) else 0.0\n\n\n            if dataset_index < len(attention_variances_demo):\n                attention_variance = attention_variances_demo[dataset_index]\n\n                if attention_variance.dim() == 0:\n                    attention_variance_specific = attention_variance.item()\n                else:\n                    attention_variance_specific = attention_variance[0].item()\n            else:\n                attention_variance_specific = 0.0\n\n\n            uncertainty_values = [\n                score_value_mcdropout,\n                #energy_score,\n                entropy_score,\n                attention_variance_mean,\n                attention_variance_specific\n            ]\n            uncertainty_features.append(uncertainty_values)\n\n\n        uncertainty_features_tensor = torch.tensor(uncertainty_features, dtype=torch.float32).to(device)\n\n\n        pred_uncertainties = qe_model(features, uncertainty_features_tensor)\n\n\n        pred_uncertainties = pred_uncertainties.squeeze(-1) \n        target = uncertainty_batch.to(device).view(-1)       \n        loss = criterion(pred_uncertainties, target)\n\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(dataloader_demo_mcdropout)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n\nprint_gpu_memory_usage(\"After_QE\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T09:32:52.020215Z","iopub.execute_input":"2025-05-29T09:32:52.020570Z","iopub.status.idle":"2025-05-29T09:32:52.713035Z","shell.execute_reply.started":"2025-05-29T09:32:52.020533Z","shell.execute_reply":"2025-05-29T09:32:52.712294Z"}},"outputs":[{"name":"stdout","text":"Before_QE: Allocated: 2.29 GB, Reserved: 2.31 GB, Free: 13.58 GB\nEpoch 1/3, Loss: 1.5553\nEpoch 2/3, Loss: 0.4600\nEpoch 3/3, Loss: 0.1359\nAfter_QE: Allocated: 2.30 GB, Reserved: 2.39 GB, Free: 13.49 GB\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"\nROOT = \"/kaggle/input/shifts-eval/eval-data/\"\n\nfile_en   = ROOT + \"reddit_eval.en\"\nfile_ru   = ROOT + \"reddit_eval.ru\"\nfile_meta = ROOT + \"reddit_eval_meta.tsv\"\n\n\nwith open(file_en, encoding=\"utf-8\") as f:\n    en_lines = [l.rstrip(\"\\n\") for l in f]\nwith open(file_ru, encoding=\"utf-8\") as f:\n    ru_lines = [l.rstrip(\"\\n\") for l in f]\n\nmeta_df = pd.read_csv(file_meta, sep=\"\\t\")\nassert len(en_lines) == len(ru_lines) == len(meta_df)\n\ndf1 = pd.DataFrame({\n    \"en\":   en_lines,\n    \"ru\":   ru_lines,\n    **{c: meta_df[c] for c in meta_df.columns}\n})\n\n\nSLANG  = r\"\\b(bruh|lol|omg|idk|smh|nah|irl|wanna|ain't|ya|tho)\\b\"\nEMOJI  = \"[\\U00010000-\\U0010ffff]\"\nNONENG = f\"[^{string.ascii_letters} ']\"              \n\ndef is_hard(row) -> bool:\n    txt   = row[\"en\"]\n    words = txt.split()\n\n   \n    if row.get(\"missp\", False) or row.get(\"slang\", False) or row.get(\"emoji\", False):\n        return True\n\n    if len(words) > 25 or len(words) < 3:\n        return True\n    if re.search(SLANG, txt.lower()) or re.search(EMOJI, txt):\n        return True\n    if len(re.findall(NONENG, txt)) / max(len(txt),1) > .15:\n        return True\n    return False\n\ndf1[\"difficulty\"] = df1.apply(is_hard, axis=1).map({True:\"hard\", False:\"easy\"})\neasy_df, hard_df = [g for _, g in df1.groupby(\"difficulty\")]\n\nprint(f\"easy = {len(easy_df)}  |  hard = {len(hard_df)}\")\n\n\neasy_100 = easy_df.iloc[:100][[\"en\",\"ru\",\"difficulty\"]]\nhard_100 = hard_df.iloc[:100][[\"en\",\"ru\",\"difficulty\"]]\n\nprint(\"Easy samples:\")\ndisplay(easy_100)\n\nprint(\"Hard samples:\")\ndisplay(hard_100)\n\n\n# tokenizer.src_lang = \"en_XX\"\n# FORCED = tokenizer.lang_code_to_id[\"ru_RU\"]\n\n# def batch_translate(model, sentences: List[str], batch_size: int = 16) -> List[str]:\n#     \n#     results = []\n#     for i in range(0, len(sentences), batch_size):\n#         batch = sentences[i : i + batch_size]\n#         enc = tokenizer(batch,\n#                         return_tensors=\"pt\",\n#                         padding=True,\n#                         truncation=True,\n#                         max_length=128).to(device)\n#         with torch.no_grad():\n#             out = model.generate(\n#                 **enc,\n#                 max_length=200,\n#                 num_beams=5,\n#                 forced_bos_token_id=FORCED\n#             )\n#         results.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n#         \n#         del enc, out\n#         torch.cuda.empty_cache()\n#     return results\n\n\n# def eval_split(split_df):\n#     en_texts = split_df[\"en\"].tolist()\n#     refs     = split_df[\"ru\"].tolist()\n\n#    \n#     with torch.no_grad():\n#         base_pred = batch_translate(model, en_texts, batch_size=16)\n\n#     \n#     ours_pred = []\n#     for i, src in enumerate(en_texts):\n#         pred = refine_translation_with_qe(\n#             src_text=src,\n#             tgt_text=refs[i],\n#             mbart_model=model,\n#             qe_model=qe_model,\n#             tokenizer=tokenizer,\n#             device=device\n#         )\n#         ours_pred.append(pred)\n#         torch.cuda.empty_cache()\n\n#     \n#     bleu_base = sacrebleu.corpus_bleu(base_pred, [refs]).score\n#     bleu_ours = sacrebleu.corpus_bleu(ours_pred, [refs]).score\n#     return bleu_base, bleu_ours\n\n# bleu_easy_b, bleu_easy_o = eval_split(easy_df)\n# bleu_hard_b, bleu_hard_o = eval_split(hard_df)\n\n# labels = [\"easy\", \"hard\"]\n# plt.bar([0,1],       [bleu_easy_b, bleu_hard_b], width=.35, label=\"baseline\")\n# plt.bar([0.35,1.35], [bleu_easy_o, bleu_hard_o], width=.35, label=\"ours\")\n# plt.xticks([0.17,1.17], labels)\n# plt.ylabel(\"BLEU ↑\")\n# plt.title(\"mBART устойчивость на Reddit dev\")\n# plt.legend()\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T09:32:52.713876Z","iopub.execute_input":"2025-05-29T09:32:52.714177Z","iopub.status.idle":"2025-05-29T09:32:52.837173Z","shell.execute_reply.started":"2025-05-29T09:32:52.714143Z","shell.execute_reply":"2025-05-29T09:32:52.836298Z"}},"outputs":[{"name":"stdout","text":"easy = 2347  |  hard = 718\nEasy samples:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                                    en  \\\n0               I’ll prolly be picking up the i Sport!   \n1    My mind just melted after seeing this, I reall...   \n2               Such a cute clitty, and pussy you have   \n3    Spider King in SKT - 2headed spider that has a...   \n4    The usual recommendations are Mirai Nikki, Hig...   \n..                                                 ...   \n125       Just unsheathe me and point me at the enemy.   \n126  But blocking payments to crypto has no effect ...   \n127  If the money in your Revolut account is illega...   \n128  It's just to force you to buy crypto in Revolu...   \n129  Lmao was Ainge supposed to move the team to ne...   \n\n                                                    ru difficulty  \n0                           Наверное, я возьму iSport!       easy  \n1    Я просто растаял, увидев это, мне бы реально х...       easy  \n2                 У тебя такие милые клиторок и киска.       easy  \n3    Король-паук в SKT–2 — это двухголовый паук, у ...       easy  \n4    Обычно рекомендуют Mirai Nikki, Higurashi, Ano...       easy  \n..                                                 ...        ...  \n125             Просто обнажи меня и направь на врага.       easy  \n126  Но блокировка платежей на криптокошельки не вл...       easy  \n127  Если деньги на твоем счете Revolut получены не...       easy  \n128  Это просто для того, чтобы принудить тебя поку...       easy  \n129  Ржака, разве Эйндж должен был перевести команд...       easy  \n\n[100 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>ru</th>\n      <th>difficulty</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I’ll prolly be picking up the i Sport!</td>\n      <td>Наверное, я возьму iSport!</td>\n      <td>easy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>My mind just melted after seeing this, I reall...</td>\n      <td>Я просто растаял, увидев это, мне бы реально х...</td>\n      <td>easy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Such a cute clitty, and pussy you have</td>\n      <td>У тебя такие милые клиторок и киска.</td>\n      <td>easy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Spider King in SKT - 2headed spider that has a...</td>\n      <td>Король-паук в SKT–2 — это двухголовый паук, у ...</td>\n      <td>easy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The usual recommendations are Mirai Nikki, Hig...</td>\n      <td>Обычно рекомендуют Mirai Nikki, Higurashi, Ano...</td>\n      <td>easy</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>125</th>\n      <td>Just unsheathe me and point me at the enemy.</td>\n      <td>Просто обнажи меня и направь на врага.</td>\n      <td>easy</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>But blocking payments to crypto has no effect ...</td>\n      <td>Но блокировка платежей на криптокошельки не вл...</td>\n      <td>easy</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>If the money in your Revolut account is illega...</td>\n      <td>Если деньги на твоем счете Revolut получены не...</td>\n      <td>easy</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>It's just to force you to buy crypto in Revolu...</td>\n      <td>Это просто для того, чтобы принудить тебя поку...</td>\n      <td>easy</td>\n    </tr>\n    <tr>\n      <th>129</th>\n      <td>Lmao was Ainge supposed to move the team to ne...</td>\n      <td>Ржака, разве Эйндж должен был перевести команд...</td>\n      <td>easy</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 3 columns</p>\n</div>"},"metadata":{}},{"name":"stdout","text":"Hard samples:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                                    en  \\\n7    If you get denied (or ignored) by every commen...   \n8                                         ambery hazel   \n10   Alternatively, Niantic is handling moved porta...   \n16   He’s a fucking weeny for bringing it up!😂🤣😂🤣😂 ...   \n17      No Kiba is the harem member lol, it's a joke 😭   \n..                                                 ...   \n410                       You need Conan as iron man 😂   \n412                                 Who’s quitting?!?!   \n414  Keep driving the price down hedge fucks, I’ll ...   \n417  But my gems are free earned from the game, i g...   \n420                                        Yes, do it.   \n\n                                                    ru difficulty  \n7    Если тебе откажут (или тебя проигнорируют) все...       hard  \n8                                        янтарно-карие       hard  \n10   Либо Niantic сейчас по-другому обрабатывает пе...       hard  \n16   Он гребаный сопляк, раз заговорил об этом! 😂🤣😂...       hard  \n17          Нет, Киба — член гарема. Лол, это шутка 😭.       hard  \n..                                                 ...        ...  \n410       Вам нужен Конан в роли Железного человека 😂.       hard  \n412                                     Кто уходит?!?!       hard  \n414  Продолжайте снижать цены, засранцы из хедж-фон...       hard  \n417  Но мои гемы бесплатные, заработанные в игре, к...       hard  \n420                                    Да, сделай это.       hard  \n\n[100 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>ru</th>\n      <th>difficulty</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7</th>\n      <td>If you get denied (or ignored) by every commen...</td>\n      <td>Если тебе откажут (или тебя проигнорируют) все...</td>\n      <td>hard</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>ambery hazel</td>\n      <td>янтарно-карие</td>\n      <td>hard</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Alternatively, Niantic is handling moved porta...</td>\n      <td>Либо Niantic сейчас по-другому обрабатывает пе...</td>\n      <td>hard</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>He’s a fucking weeny for bringing it up!😂🤣😂🤣😂 ...</td>\n      <td>Он гребаный сопляк, раз заговорил об этом! 😂🤣😂...</td>\n      <td>hard</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>No Kiba is the harem member lol, it's a joke 😭</td>\n      <td>Нет, Киба — член гарема. Лол, это шутка 😭.</td>\n      <td>hard</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>410</th>\n      <td>You need Conan as iron man 😂</td>\n      <td>Вам нужен Конан в роли Железного человека 😂.</td>\n      <td>hard</td>\n    </tr>\n    <tr>\n      <th>412</th>\n      <td>Who’s quitting?!?!</td>\n      <td>Кто уходит?!?!</td>\n      <td>hard</td>\n    </tr>\n    <tr>\n      <th>414</th>\n      <td>Keep driving the price down hedge fucks, I’ll ...</td>\n      <td>Продолжайте снижать цены, засранцы из хедж-фон...</td>\n      <td>hard</td>\n    </tr>\n    <tr>\n      <th>417</th>\n      <td>But my gems are free earned from the game, i g...</td>\n      <td>Но мои гемы бесплатные, заработанные в игре, к...</td>\n      <td>hard</td>\n    </tr>\n    <tr>\n      <th>420</th>\n      <td>Yes, do it.</td>\n      <td>Да, сделай это.</td>\n      <td>hard</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"\n\ntest_texts = [\n    \"Think of it as macros/python with a GUI to do a lot the work instead of writing code.\",\n    \"how much did uy save up before leaving.\",\n    \"Why We Sleep - This book is really eye-opening and really makes you evaluate your relationship with sleep.\",\n    \"But I know DFV have diamond balls so if he exercises, he will hold that shares tight as fuck.\",\n    \"Ive never heard of this.. Would it be wise to underfoot a 3070 or a 2070 super?\",\n    \"2/8 and 2/24 and still waiting.🙄\",\n    \"His JP enhancements were pretty terrible though, he should be a bit better in GL\",\n    \"You will get lower hashrates, but your gui should be more usable.\",\n    \"If you get enough critrate go for the atlas\",\n    \"Yeah that makes sense, ty.\",\n    \"I remember when I originally watched the film being completely sucked in, but it hasn't aged well.\",\n    \"no he makes an eeeeee sound when u shoot we probably shot the door down on him and he is in pain plus the bullies didn't know what happens out of the school all they know is the signal tower the eye and the janitor\",\n    \"Trash coin lol, I'm a lover of shitcoins but haven't touched bsc lol each to their own 😂\",\n    \"The screams of pain in the background did not feel forced at all and added to the already superb vocals\",\n    \"I am really lazy so finding this is a god send lmao.\",\n    \"It's very intuitive & works really well, IMO.\",\n    \"Yes\",\n    \"My name is Oleg\",\n    \"How are you?\",\n    \"This model is not very good.\",\n    \"A random sentence to check quality.\"\n]\n\ntest_texts_ru = [\n    \"Представь себе, что это макрос/python с пользовательским интерфейсом, в котором ты можешь много работать вместо того, чтобы писать код.\",\n    \"Сколько тебе удалось накопить перед тем, как ты ушел?\",\n    \"Почему мы спим: это действительно познавательная книга, которая реально позволяет вам оценить ваши взаимоотношения со сном.\",\n    \"Но я знаю, что у DFV алмазные яйца, и если он их реализует, он будет держать эти акции так крепко, что хрен оторвешь.\",\n    \"Я об этом никогда не слышал... Имеет ли смысл делать андервольтинг на 3070 или 2070 Super?\",\n    \"8.02, 24.02, и все еще жду.🙄\",\n    \"Хотя его усовершенствования в JP были довольно ужасны, в GL ему должно быть немного получше.\",\n    \"Ваш хешрейт будет ниже, но пользовательский интерфейс будет более удобным для использования.\",\n    \"Если получишь достаточно CRIT-рейтинга, иди за атласом\",\n    \"Да, в этом есть смысл, спасибо.\",\n    \"Я помню, что, когда смотрел фильм в первый раз, меня совершенно засосало, но проверку временем он не прошел.\",\n    \"Нет, он издает звук «иииии», когда ты стреляешь. Мы, наверное, проигнорировали его, и ему больно. Плюс, те, кто его травили, не знали о том, что происходит за пределами школы. Все, о чем они знают, это сигнальная башня, глаз и уборщик.\",\n    \"Мусорный коин, лол. Мне нравятся говнокоины, но к BSC я не притрагивался, лол. Каждому свое 😂.\",\n    \"Крики от боли на фоне совсем не казались имитацией и дополнили и без того превосходный вокал\",\n    \"Я реально ленивый, поэтому найти это — прям божий дар, хаха.\",\n    \"По-моему, это очень интуитивно понятный инструмент, который действительно хорошо работает.\",\n    \"Да\",\n    \"Мое имя Олег\",\n    \"Как вы?\",\n    \"Эта модель не очень хороша\",\n    \"Случайное предложение для проверки качества\"    \n]\n\nru_texts_demo1 = [None] * len(test_texts)\n\n\nprint(\"Number of lines in texts:\", len(test_texts))\n\n\n# if not ru_texts_demo1:\n#     for i, txt in enumerate(test_texts):\n\n#         inputs = tokenizer(txt, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n#         inputs = {key: value.to(device) for key, value in inputs.items()}\n\n\n#         with torch.no_grad():\n#             gen_ids = model.generate(\n#                 **inputs,\n#                 max_length=200,\n#                 num_beams=5,  \n#                 do_sample=False, \n#                 forced_bos_token_id=tokenizer.lang_code_to_id[\"ru_RU\"], \n#                 repetition_penalty=1.1\n#             )\n\n\n#         ru_translation = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n#         ru_texts_demo1.append(ru_translation)\n\n\n#         input_text = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n\n#         del inputs, gen_ids\n\n#     gc.collect()\n\n\n#     with open(ru_texts_file, \"w\", encoding=\"utf-8\") as f:\n#         for line in ru_texts_demo:\n#             f.write(line + \"\\n\")\n\n#     #####-----------------------------------------------\n\n\n# print_gpu_memory_usage(\"After processing lines\")\n\n# test_texts.extend(hard_100[\"en\"])\n# test_texts_ru.extend(hard_100[\"ru\"])\n\n# # easy_100 = easy_df.iloc[:100][[\"en\",\"ru\",\"difficulty\"]]\n# # hard_100 = hard_df.iloc[:100][[\"en\",\"ru\",\"difficulty\"]]\n\n# scores_demo = extract_attention_uncertainty(test_texts, model, tokenizer, device)\n\n\n# inputs = tokenizer(test_texts, padding=True, truncation=True, return_tensors='pt').to(device)\n\n\n# (translated_ids1, scores_dropout1,\n#  entropy_scores_demo1, attention_variances_demo1,\n#  hidden_variances_demo1) = mc_dropout_inference(model, inputs, n=10, batch_size=1, device=device)\n\n# # print(\"scores_dropout1 \", scores_dropout1)\n# # print(\"energy_scores_demo1 \", energy_scores_demo1)\n# # print(\"entropy_scores_demo1 \", entropy_scores_demo1)\n# # print(\"attention_variances_demo1 \", attention_variances_demo1)\n# # print(\"hidden_variances_demo1 \", hidden_variances_demo1)\n\n\n\n\n\n\n# for i, text in enumerate(test_texts):\n#     \n#     inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n#     with torch.no_grad():\n#         outputs = model.generate(\n#             **inputs,\n#             max_length=200,\n#             num_beams=10,\n#             do_sample=False,\n#             forced_bos_token_id=tokenizer.lang_code_to_id[\"ru_RU\"],\n#             repetition_penalty=1.1\n#         )\n#     original_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n#     \n#     tgt_text = test_texts_ru[i]\n\n#    \n#     refined_translation = refine_translation_with_qe(\n#         src_text=text,\n#         tgt_text=tgt_text,\n#         mbart_model=model,\n#         qe_model=qe_model,\n#         tokenizer=tokenizer,\n#         device=device\n#     )\n\n\ntest_texts.extend(hard_100[\"en\"])\ntest_texts_ru.extend(hard_100[\"ru\"])\n\n\n\n\ndecode_kwargs = dict(\n    max_length=200, num_beams=4, do_sample=True,\n    top_k=50, top_p=0.9, temperature=0.8,\n    num_return_sequences=1,\n    forced_bos_token_id=tokenizer.lang_code_to_id[\"ru_RU\"],\n    repetition_penalty=1.1\n)\nthreshold = 0.9\n\n\nbenchmark = {\n    \"no_adapt\":   {\"train_gpu_h\": 0.0, \"epoch_times\": [], \"peak_mem\": 0, \"infer_times\": []},\n    \"proposed\":   {\"train_gpu_h\": 0.0, \"epoch_times\": [], \"peak_mem\": 0, \"infer_times\": []},\n    \"full_finet\": {\"train_gpu_h\": 0.0, \"epoch_times\": [], \"peak_mem\": 0, \"infer_times\": []},\n}\ndef record_memory(tag):\n    if torch.cuda.is_available():\n        used = torch.cuda.max_memory_reserved() / (1024**3)\n        benchmark[tag][\"peak_mem\"] = max(benchmark[tag][\"peak_mem\"], used)\n\n\ntorch.cuda.reset_peak_memory_stats()\nstart_inf = time.time()\nfor text in test_texts:\n    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        _ = model.generate(\n            **inputs,\n            max_length=200, num_beams=10,\n            forced_bos_token_id=tokenizer.lang_code_to_id[\"ru_RU\"]\n        )\nend_inf = time.time()\nbenchmark[\"no_adapt\"][\"infer_times\"].append((end_inf - start_inf) / len(test_texts))\nrecord_memory(\"no_adapt\")\n\n\n\nRISK_BUFFER = []\nfor src, tgt in zip(test_texts, test_texts_ru):\n\n    inputs = tokenizer(src, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        _ = model.generate(**inputs, **decode_kwargs)\n\n    attn_entropy = extract_attention_uncertainty([src], model, tokenizer, device)[0].item()\n    feat = extract_mbart_features(src, model, tokenizer, device)\n    with torch.no_grad():\n        qe_unc = qe_model(feat, torch.zeros(1,4, device=device)).item()\n    risk = 0.7 * attn_entropy + 0.3 * qe_unc\n    if risk > threshold:\n        RISK_BUFFER.append((src, tgt))\n\n\ntorch.cuda.reset_peak_memory_stats()\nt0 = time.time()\nprop_epochs = full_finetune(\n    model, tokenizer, device,\n    all_pairs=RISK_BUFFER,\n    epochs=2,\n    batch_size=8,\n    lr=1e-5,\n    lbl_smth=0.1\n)\ndt = time.time() - t0\nbenchmark[\"proposed\"][\"train_gpu_h\"] = dt / 3600\nbenchmark[\"proposed\"][\"epoch_times\"] = prop_epochs\nrecord_memory(\"proposed\")\n\n\ntorch.cuda.reset_peak_memory_stats()\nstart_inf = time.time()\nfor text in test_texts:\n    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        _ = model.generate(**inputs, **decode_kwargs)\nend_inf = time.time()\nbenchmark[\"proposed\"][\"infer_times\"].append((end_inf - start_inf) / len(test_texts))\nrecord_memory(\"proposed\")\n\n\ntorch.cuda.reset_peak_memory_stats()\nt0 = time.time()\nfull_epochs = full_finetune(\n    model, tokenizer, device,\n    all_pairs=list(zip(test_texts, test_texts_ru)),\n    epochs=2,\n    batch_size=10,\n    lr=1e-5,\n    lbl_smth=0.1\n)\ndt_full = time.time() - t0\nbenchmark[\"full_finet\"][\"train_gpu_h\"] = dt_full / 3600\nbenchmark[\"full_finet\"][\"epoch_times\"] = full_epochs\nrecord_memory(\"full_finet\")\n\n\nstart_inf = time.time()\nfor text in test_texts:\n    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        _ = model.generate(\n            **inputs,\n            max_length=200, num_beams=10,\n            forced_bos_token_id=tokenizer.lang_code_to_id[\"ru_RU\"]\n        )\nend_inf = time.time()\nbenchmark[\"full_finet\"][\"infer_times\"].append((end_inf - start_inf) / len(test_texts))\nrecord_memory(\"full_finet\")\n\n\nrows = []\nname_map = {\n    \"no_adapt\":   \"Без адаптации\",\n    \"proposed\":   \"Предлагаемый метод\",\n    \"full_finet\": \"Полный fine-tuning\"\n}\nfor method, stats in benchmark.items():\n\n    if stats[\"epoch_times\"]:\n        avg_min = int((sum(stats[\"epoch_times\"]) / len(stats[\"epoch_times\"])) // 60)\n        epoch_str = f\"{avg_min} мин\"\n    else:\n        epoch_str = \"—\"\n\n    gpu_h = f\"{stats['train_gpu_h']:.1f}\" if method != \"no_adapt\" else \"—\"\n\n    mem = f\"{stats['peak_mem']:.1f}\"\n\n    inf = f\"{stats['infer_times'][0]*1000:.0f}\" if stats[\"infer_times\"] else \"—\"\n\n    rows.append({\n        \"Метод\":                name_map[method],\n        \"GPU-часы\":            gpu_h,\n        \"Время эпохи\":         epoch_str,\n        \"Память (ГБ)\":         mem,\n        \"Инференс (мс/пример)\": inf\n    })\n\ndf_bench = pd.DataFrame(rows)\nprint(df_bench.to_markdown(index=False))    \n\n\n    # print(f\"--- Sample {i} ---\")\n    # print(f\"Input: {text}\\n\")\n    # print(f\"Reference Translation: {tgt_text}\\n\")\n    # print(f\"Original Translation (MBart): {original_translation}\\n\")\n    # print(f\"Refined Translation (MBart + QE): {refined_translation}\\n\\n\")\n\n\n\n\n    # inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n    # with torch.no_grad():\n\n    #     outputs = model(**inputs, output_attentions=True, return_dict=True)\n\n    # attentions = outputs.encoder_attentions \n    # avg_attention = torch.mean(torch.stack(attentions), dim=(0, 1)) \n    # attn_entropy = -torch.sum(avg_attention * torch.log(avg_attention + 1e-9), dim=-1).mean()\n\n\n    # score_mcdropout1 = scores_dropout1[i].item() if i < len(scores_dropout1) else None\n\n    # energy_score_value1 = energy_scores_demo1[i] if i < len(energy_scores_demo1) else None\n\n\n    # entropy_value1 = entropy_scores_demo1[i] if i < len(entropy_scores_demo1) else None\n\n\n    # avg_variance = attention_variances_demo1[i].mean().item()\n    # specific_variance = attention_variances_demo1[i].item()\n\n\n    # input_ids = inputs[\"input_ids\"].float() \n\n\n    # mean_input_ids = torch.mean(input_ids, dim=0)  \n    # cov_input_ids = torch.cov(input_ids.T) \n\n\n    # cosine_similarity_value = cosine_similarity_score(input_ids, mean_input_ids.unsqueeze(0))\n\n    # print(f\"Uncertainty via attention entropy (from QEDataset): {attn_entropy:.4f}\")\n    # print()\n    # print(f\"Attention Variance (mean): {avg_variance:.4f}\")\n    # print()\n    # print(f\"Attention Variance (index 0): {specific_variance:.4f}\")\n    # print()\n\n    # if score_mcdropout1 is not None:\n    #     print(f\"Uncertainty via MC Dropout (from QEDataset): {score_mcdropout1:.4f}\")\n    #     print()\n    # else:\n    #     print(\"Warning: MC Dropout Score is missing.\")\n        \n    # if energy_score_value1 is not None:\n    #     print(f\"Energy Score: {energy_score_value1:.4f}\")\n    #     print()\n    # else:\n    #     print(\"Warning: Energy Score is missing.\")\n        \n    # if entropy_value1 is not None:\n    #     print(f\"Entropy Score: {entropy_value1:.4f}\")\n    #     print()\n    # else:\n    #     print(\"Warning: Entropy Score is missing.\")\n    # print(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T09:32:52.838156Z","iopub.execute_input":"2025-05-29T09:32:52.838460Z","iopub.status.idle":"2025-05-29T09:35:19.736327Z","shell.execute_reply.started":"2025-05-29T09:32:52.838430Z","shell.execute_reply":"2025-05-29T09:35:19.735194Z"}},"outputs":[{"name":"stdout","text":"Number of lines in texts: 21\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-d832ce561ba1>\u001b[0m in \u001b[0;36m<cell line: 209>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_peak_memory_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m prop_epochs = full_finetune(\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0mall_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRISK_BUFFER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'full_finetune' is not defined"],"ename":"NameError","evalue":"name 'full_finetune' is not defined","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"\n\nprint(\"Вывод содержимого QEDataset:\")\nstart_index = 0\nfor i in range(start_index, min(30, len(dataset_demo_mcdropout))):\n    en_text, ru_text, score_value = dataset_demo[i]\n    en_text_mcdropout, ru_text_mcdropout, score_tensor_mcdropout = dataset_demo_mcdropout[i]\n    score_value_mcdropout = score_tensor_mcdropout.item()\n\n    refined_translation = refine_translation_with_qe(en_text, ru_text, model, qe_model, tokenizer, device)\n\n    print(f\"--- Образец {i} ---\")\n    print(f\"English: {en_text}\")\n    print()\n    print()\n    print(f\"Original Translation (ru_lines): {ru_lines[i]}\")\n    print()\n    print()\n    print(\"-\" * 100)\n    print()\n    print(f\"Refined Translation (MBart + QE): {refined_translation}\")\n    \n    print(\"-\" * 100)\n    print()\n    print()\n    print(f\"Uncertainty через энтропию внимания (из QEDataset): {score_value:.4f}\")\n    print()\n    print(f\"Uncertainty через MC Dropout (из QEDataset): {score_value_mcdropout:.4f}\")\n    print()\n\n    # if i < len(energy_scores_demo):\n    #     print(f\"Energy Score: {energy_scores_demo[i]:.4f}\")\n    #     print()\n    # else:\n    #     print(\"⚠ Warning: Energy Score не найден для этого примера.\")\n        \n\n    if i < len(entropy_scores_demo):\n        print(f\"Entropy Score: {entropy_scores_demo[i]:.4f}\")\n        print()\n    else:\n        print(\"⚠ Warning: Entropy Score не найден для этого примера.\")\n\n\n    avg_variance = attention_variances_demo[i].mean().item()\n    print(f\"Attention Variance (mean): {avg_variance:.4f}\")\n    print()\n\n    desired_index = 0\n    specific_variance = attention_variances_demo[i].item()\n    print(f\"Attention Variance (index {desired_index}): {specific_variance:.4f}\")\n    print()\n\n\n\n    input_tensor = inputs[\"input_ids\"].float()\n\n    if input_tensor.shape[0] < 4:\n        noise = torch.randn((4 - input_tensor.shape[0], input_tensor.shape[1]), device=input_tensor.device) * 1e-6\n        input_tensor = torch.cat([input_tensor, noise], dim=0)\n    \n\n    if i >= input_tensor.shape[0]:\n        cosine_similarity_value = torch.tensor(0.0, dtype=torch.float32, device=device)  # Теперь это тензор\n    else:\n        h = input_tensor[i].unsqueeze(0) if input_tensor[i].dim() == 1 else input_tensor[i]\n        h_train = torch.mean(input_tensor.float(), dim=0, keepdim=True)\n        cosine_similarity_value = cosine_similarity_score(h, h_train)\n\n    print(f\"Cosine Similarity (OOD Score): {cosine_similarity_value:.4f}\")\n\n    \n    print(\"=\" * 100)\n    print(\"=\" * 100)\n    print(\"=\" * 100)\n    print(\"=\" * 100)\n","metadata":{"trusted":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2025-05-29T09:35:19.736778Z","iopub.status.idle":"2025-05-29T09:35:19.737038Z","shell.execute_reply":"2025-05-29T09:35:19.736935Z"}},"outputs":[],"execution_count":null}]}